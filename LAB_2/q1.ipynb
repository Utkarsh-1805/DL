{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a61d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, \n",
    "                                          download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, \n",
    "                                         download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Input shape: {train_dataset[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7db366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    img = images[i].squeeze().numpy()\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(f'Label: {labels[i].item()}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7cc8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, activation='relu', fc_units=128, dropout_rate=0.25):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Max pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, fc_units)\n",
    "        self.fc2 = nn.Linear(fc_units, 10)\n",
    "        \n",
    "        # Set activation function\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = torch.sigmoid\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        elif activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "        else:\n",
    "            self.activation = F.relu\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv layers with activation\n",
    "        x = self.activation(self.conv1(x))\n",
    "        x = self.activation(self.conv2(x))\n",
    "        \n",
    "        # Max pooling\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 64 * 14 * 14)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8277c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, activation='relu', hidden_units=[256, 128], \n",
    "                 use_batch_norm=True, dropout_rate=0.25):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Create layers dynamically\n",
    "        layers = []\n",
    "        input_size = 784\n",
    "        \n",
    "        for units in hidden_units:\n",
    "            layers.append(nn.Linear(input_size, units))\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(units))\n",
    "            input_size = units\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList(layers)\n",
    "        self.output_layer = nn.Linear(input_size, 10)\n",
    "        \n",
    "        # Set activation function\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = torch.sigmoid\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        elif activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "        else:\n",
    "            self.activation = F.relu\n",
    "        \n",
    "        # Dropout\n",
    "        if dropout_rate > 0:\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten input\n",
    "        x = x.view(-1, 784)\n",
    "        \n",
    "        # Pass through hidden layers\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "            if not isinstance(layer, nn.BatchNorm1d):\n",
    "                x = self.activation(x)\n",
    "                if self.dropout is not None:\n",
    "                    x = self.dropout(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b6361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def train_model(model, device, train_loader, test_loader, optimizer, epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, device, train_loader, optimizer, criterion)\n",
    "        val_loss, val_acc = evaluate(model, device, test_loader, criterion)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} - '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4695ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TASK 1: ACTIVATION FUNCTION CHALLENGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "activation_results = []\n",
    "activation_histories = {}\n",
    "\n",
    "activations = ['sigmoid', 'tanh', 'relu']\n",
    "\n",
    "for act in activations:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training CNN with {act.upper()} activation\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    model = CNN(activation=act, fc_units=128, dropout_rate=0.25).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    history = train_model(model, device, train_loader, test_loader, optimizer, epochs=10)\n",
    "    \n",
    "    # Store results\n",
    "    final_acc = history['val_acc'][-1]\n",
    "    activation_results.append({\n",
    "        'Activation': act,\n",
    "        'Optimizer': 'Adam',\n",
    "        'Epochs': 10,\n",
    "        'Final Train Accuracy': f\"{history['train_acc'][-1]:.2f}%\",\n",
    "        'Final Test Accuracy': f\"{final_acc:.2f}%\"\n",
    "    })\n",
    "    \n",
    "    activation_histories[act] = history\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 1 RESULTS\")\n",
    "print(\"=\"*60)\n",
    "df_activation = pd.DataFrame(activation_results)\n",
    "print(df_activation.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ceec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training Accuracy\n",
    "for act in activations:\n",
    "    axes[0, 0].plot(activation_histories[act]['train_acc'], label=act.upper(), marker='o')\n",
    "axes[0, 0].set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy\n",
    "for act in activations:\n",
    "    axes[0, 1].plot(activation_histories[act]['val_acc'], label=act.upper(), marker='o')\n",
    "axes[0, 1].set_title('Validation Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training Loss\n",
    "for act in activations:\n",
    "    axes[1, 0].plot(activation_histories[act]['train_loss'], label=act.upper(), marker='o')\n",
    "axes[1, 0].set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "for act in activations:\n",
    "    axes[1, 1].plot(activation_histories[act]['val_loss'], label=act.upper(), marker='o')\n",
    "axes[1, 1].set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b214b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TASK 2: OPTIMIZER SHOWDOWN (Using ReLU)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "optimizer_results = []\n",
    "optimizer_histories = {}\n",
    "\n",
    "# Define optimizer configurations\n",
    "optimizer_configs = {\n",
    "    'SGD': lambda params: optim.SGD(params, lr=0.01),\n",
    "    'SGD_Momentum': lambda params: optim.SGD(params, lr=0.01, momentum=0.9),\n",
    "    'Adam': lambda params: optim.Adam(params, lr=0.001)\n",
    "}\n",
    "\n",
    "for opt_name, opt_func in optimizer_configs.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training CNN with {opt_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    model = CNN(activation='relu', fc_units=128, dropout_rate=0.25).to(device)\n",
    "    optimizer = opt_func(model.parameters())\n",
    "    \n",
    "    history = train_model(model, device, train_loader, test_loader, optimizer, epochs=10)\n",
    "    \n",
    "    # Store results\n",
    "    final_acc = history['val_acc'][-1]\n",
    "    optimizer_results.append({\n",
    "        'Activation': 'ReLU',\n",
    "        'Optimizer': opt_name,\n",
    "        'Epochs': 10,\n",
    "        'Final Train Accuracy': f\"{history['train_acc'][-1]:.2f}%\",\n",
    "        'Final Test Accuracy': f\"{final_acc:.2f}%\"\n",
    "    })\n",
    "    \n",
    "    optimizer_histories[opt_name] = history\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 2 RESULTS\")\n",
    "print(\"=\"*60)\n",
    "df_optimizer = pd.DataFrame(optimizer_results)\n",
    "print(df_optimizer.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd129ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training Accuracy\n",
    "for opt_name in optimizer_configs.keys():\n",
    "    axes[0, 0].plot(optimizer_histories[opt_name]['train_acc'], label=opt_name, marker='o')\n",
    "axes[0, 0].set_title('Training Accuracy - Optimizer Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy\n",
    "for opt_name in optimizer_configs.keys():\n",
    "    axes[0, 1].plot(optimizer_histories[opt_name]['val_acc'], label=opt_name, marker='o')\n",
    "axes[0, 1].set_title('Validation Accuracy - Optimizer Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training Loss\n",
    "for opt_name in optimizer_configs.keys():\n",
    "    axes[1, 0].plot(optimizer_histories[opt_name]['train_loss'], label=opt_name, marker='o')\n",
    "axes[1, 0].set_title('Training Loss - Optimizer Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "for opt_name in optimizer_configs.keys():\n",
    "    axes[1, 1].plot(optimizer_histories[opt_name]['val_loss'], label=opt_name, marker='o')\n",
    "axes[1, 1].set_title('Validation Loss - Optimizer Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da15026",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TASK 3: BATCH NORMALIZATION & DROPOUT IMPACT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "task3_results = []\n",
    "task3_histories = {}\n",
    "\n",
    "# Scenario configurations\n",
    "scenarios = [\n",
    "    {'name': 'Without BN, Dropout=0', 'use_bn': False, 'dropout': 0.0},\n",
    "    {'name': 'Without BN, Dropout=0.1', 'use_bn': False, 'dropout': 0.1},\n",
    "    {'name': 'With BN, Dropout=0.25', 'use_bn': True, 'dropout': 0.25}\n",
    "]\n",
    "\n",
    "for scenario in scenarios:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training MLP: {scenario['name']}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    model = MLP(\n",
    "        activation='relu',\n",
    "        hidden_units=[256, 128],\n",
    "        use_batch_norm=scenario['use_bn'],\n",
    "        dropout_rate=scenario['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    history = train_model(model, device, train_loader, test_loader, optimizer, epochs=15)\n",
    "    \n",
    "    # Store results\n",
    "    final_acc = history['val_acc'][-1]\n",
    "    task3_results.append({\n",
    "        'Scenario': scenario['name'],\n",
    "        'Batch Norm': 'Yes' if scenario['use_bn'] else 'No',\n",
    "        'Dropout': scenario['dropout'],\n",
    "        'Final Train Accuracy': f\"{history['train_acc'][-1]:.2f}%\",\n",
    "        'Final Test Accuracy': f\"{final_acc:.2f}%\"\n",
    "    })\n",
    "    \n",
    "    task3_histories[scenario['name']] = history\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 3 RESULTS\")\n",
    "print(\"=\"*60)\n",
    "df_task3 = pd.DataFrame(task3_results)\n",
    "print(df_task3.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab7b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training Accuracy\n",
    "for scenario_name in task3_histories.keys():\n",
    "    axes[0, 0].plot(task3_histories[scenario_name]['train_acc'], label=scenario_name, marker='o')\n",
    "axes[0, 0].set_title('Training Accuracy - BN & Dropout Impact', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "axes[0, 0].legend(fontsize=8)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy\n",
    "for scenario_name in task3_histories.keys():\n",
    "    axes[0, 1].plot(task3_histories[scenario_name]['val_acc'], label=scenario_name, marker='o')\n",
    "axes[0, 1].set_title('Validation Accuracy - BN & Dropout Impact', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].legend(fontsize=8)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training Loss\n",
    "for scenario_name in task3_histories.keys():\n",
    "    axes[1, 0].plot(task3_histories[scenario_name]['train_loss'], label=scenario_name, marker='o')\n",
    "axes[1, 0].set_title('Training Loss - BN & Dropout Impact', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend(fontsize=8)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "for scenario_name in task3_histories.keys():\n",
    "    axes[1, 1].plot(task3_histories[scenario_name]['val_loss'], label=scenario_name, marker='o')\n",
    "axes[1, 1].set_title('Validation Loss - BN & Dropout Impact', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend(fontsize=8)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886f34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING REQUIRED MODELS FROM TABLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "required_models_results = []\n",
    "required_models_histories = {}\n",
    "\n",
    "# CNN-1: 128 FC, Adam, 10 epochs\n",
    "print(\"\\n--- Training CNN-1 ---\")\n",
    "cnn1 = CNN(activation='relu', fc_units=128, dropout_rate=0.25).to(device)\n",
    "optimizer_cnn1 = optim.Adam(cnn1.parameters(), lr=0.001)\n",
    "history_cnn1 = train_model(cnn1, device, train_loader, test_loader, optimizer_cnn1, epochs=10)\n",
    "required_models_results.append({\n",
    "    'Model': 'CNN-1',\n",
    "    'FC Layer': '128',\n",
    "    'Optimizer': 'Adam',\n",
    "    'Epochs': 10,\n",
    "    'Accuracy': f\"{history_cnn1['val_acc'][-1]:.2f}%\"\n",
    "})\n",
    "required_models_histories['CNN-1'] = history_cnn1\n",
    "\n",
    "# MLP-1: 512-256-128, SGD, 20 epochs\n",
    "print(\"\\n--- Training MLP-1 ---\")\n",
    "mlp1 = MLP(activation='relu', hidden_units=[512, 256, 128], \n",
    "           use_batch_norm=True, dropout_rate=0.25).to(device)\n",
    "optimizer_mlp1 = optim.SGD(mlp1.parameters(), lr=0.01)\n",
    "history_mlp1 = train_model(mlp1, device, train_loader, test_loader, optimizer_mlp1, epochs=20)\n",
    "required_models_results.append({\n",
    "    'Model': 'MLP-1',\n",
    "    'FC Layer': '512-256-128',\n",
    "    'Optimizer': 'SGD',\n",
    "    'Epochs': 20,\n",
    "    'Accuracy': f\"{history_mlp1['val_acc'][-1]:.2f}%\"\n",
    "})\n",
    "required_models_histories['MLP-1'] = history_mlp1\n",
    "\n",
    "# MLP-2: 256, Adam, 15 epochs\n",
    "print(\"\\n--- Training MLP-2 ---\")\n",
    "mlp2 = MLP(activation='relu', hidden_units=[256], \n",
    "           use_batch_norm=True, dropout_rate=0.25).to(device)\n",
    "optimizer_mlp2 = optim.Adam(mlp2.parameters(), lr=0.001)\n",
    "history_mlp2 = train_model(mlp2, device, train_loader, test_loader, optimizer_mlp2, epochs=15)\n",
    "required_models_results.append({\n",
    "    'Model': 'MLP-2',\n",
    "    'FC Layer': '256',\n",
    "    'Optimizer': 'Adam',\n",
    "    'Epochs': 15,\n",
    "    'Accuracy': f\"{history_mlp2['val_acc'][-1]:.2f}%\"\n",
    "})\n",
    "required_models_histories['MLP-2'] = history_mlp2\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REQUIRED MODELS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "df_required = pd.DataFrame(required_models_results)\n",
    "print(df_required.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79421508",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPREHENSIVE EXPERIMENT RESULTS TABLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Add Task 1 results\n",
    "for i, result in enumerate(activation_results, 1):\n",
    "    all_results.append({\n",
    "        'Experiment': f'Task1-{i}',\n",
    "        'Activation': result['Activation'],\n",
    "        'Optimizer': result['Optimizer'],\n",
    "        'Epochs': result['Epochs'],\n",
    "        'Final Accuracy': result['Final Test Accuracy']\n",
    "    })\n",
    "\n",
    "# Add Task 2 results\n",
    "for i, result in enumerate(optimizer_results, 1):\n",
    "    all_results.append({\n",
    "        'Experiment': f'Task2-{i}',\n",
    "        'Activation': result['Activation'],\n",
    "        'Optimizer': result['Optimizer'],\n",
    "        'Epochs': result['Epochs'],\n",
    "        'Final Accuracy': result['Final Test Accuracy']\n",
    "    })\n",
    "\n",
    "df_all_results = pd.DataFrame(all_results)\n",
    "print(df_all_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a84290",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY AND OBSERVATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. ACTIVATION FUNCTION ANALYSIS:\")\n",
    "print(\"   - Sigmoid: Prone to vanishing gradients, slower convergence\")\n",
    "print(\"   - Tanh: Better than sigmoid, zero-centered, but still slower than ReLU\")\n",
    "print(\"   - ReLU: Fastest convergence, no vanishing gradient for positive values\")\n",
    "\n",
    "print(\"\\n2. OPTIMIZER ANALYSIS:\")\n",
    "print(\"   - SGD: Stable but slower convergence\")\n",
    "print(\"   - SGD with Momentum: Smoother convergence, handles local minima better\")\n",
    "print(\"   - Adam: Fastest convergence with adaptive learning rates\")\n",
    "\n",
    "print(\"\\n3. BATCH NORMALIZATION & DROPOUT:\")\n",
    "print(\"   - Without BN/Dropout: May overfit, unstable training\")\n",
    "print(\"   - Dropout helps prevent overfitting\")\n",
    "print(\"   - Batch Normalization speeds up training and improves stability\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PRACTICAL COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33416b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best performing model\n",
    "torch.save(cnn1.state_dict(), 'best_cnn_model.pth')\n",
    "print(\"\\nBest CNN model saved as 'best_cnn_model.pth'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
