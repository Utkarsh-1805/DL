{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f06340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Install Required Libraries\n",
    "# ============================================================\n",
    "# Run this cell first to install all necessary packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install scikit-learn tqdm matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f0831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Import Libraries and Setup\n",
    "# ============================================================\n",
    "# Import all necessary libraries and configure the environment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb9ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Define LeNet-5 Architecture\n",
    "# ============================================================\n",
    "# LeNet-5: Classic CNN architecture with 2 conv layers\n",
    "def create_lenet5(num_classes=10):\n",
    "    \"\"\"LeNet-5 Architecture\"\"\"\n",
    "    model = nn.Sequential(\n",
    "        # Conv Layer 1\n",
    "        nn.Conv2d(3, 6, kernel_size=5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        \n",
    "        # Conv Layer 2\n",
    "        nn.Conv2d(6, 16, kernel_size=5),\n",
    "        nn.ReLU(),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        \n",
    "        # Flatten\n",
    "        nn.Flatten(),\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        nn.Linear(16 * 5 * 5, 120),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(120, 84),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(84, num_classes)\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eef0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Define AlexNet Architecture\n",
    "# ============================================================\n",
    "# AlexNet: Deep CNN with 5 conv layers and dropout\n",
    "def create_alexnet(num_classes=10):\n",
    "    \"\"\"AlexNet Architecture (Simplified for smaller images)\"\"\"\n",
    "    model = nn.Sequential(\n",
    "        # Conv Block 1\n",
    "        nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        \n",
    "        # Conv Block 2\n",
    "        nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        \n",
    "        # Conv Block 3\n",
    "        nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        # Conv Block 4\n",
    "        nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        # Conv Block 5\n",
    "        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        \n",
    "        # Flatten and FC layers\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(256 * 4 * 4, 4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(4096, num_classes)\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c22e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Define VGGNet Architecture\n",
    "# ============================================================\n",
    "# VGGNet: Very deep network with small 3x3 filters\n",
    "def create_vggnet(num_classes=10):\n",
    "    \"\"\"VGG-16 Architecture (Simplified)\"\"\"\n",
    "    model = nn.Sequential(\n",
    "        # Block 1\n",
    "        nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        \n",
    "        # Block 2\n",
    "        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        \n",
    "        # Block 3\n",
    "        nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        \n",
    "        # Block 4\n",
    "        nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        \n",
    "        # Flatten and FC\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(512 * 2 * 2, 4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(4096, num_classes)\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19213db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Define ResNet Building Blocks\n",
    "# ============================================================\n",
    "# ResNet: Helper function to create residual blocks\n",
    "def create_resnet_block(in_channels, out_channels, stride=1):\n",
    "    \"\"\"Basic ResNet Block\"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3433e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Define ResNet-50 Architecture\n",
    "# ============================================================\n",
    "# ResNet-50: 50-layer network with residual connections\n",
    "def create_resnet50(num_classes=10):\n",
    "    \"\"\"ResNet-50 Architecture (Simplified)\"\"\"\n",
    "    class ResNet50(nn.Module):\n",
    "        def __init__(self, num_classes):\n",
    "            super(ResNet50, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "            self.relu = nn.ReLU()\n",
    "            \n",
    "            # Residual blocks\n",
    "            self.layer1 = self._make_layer(64, 64, 3)\n",
    "            self.layer2 = self._make_layer(64, 128, 4, stride=2)\n",
    "            self.layer3 = self._make_layer(128, 256, 6, stride=2)\n",
    "            self.layer4 = self._make_layer(256, 512, 3, stride=2)\n",
    "            \n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "        def _make_layer(self, in_channels, out_channels, blocks, stride=1):\n",
    "            layers = []\n",
    "            layers.append(create_resnet_block(in_channels, out_channels, stride))\n",
    "            for _ in range(1, blocks):\n",
    "                layers.append(create_resnet_block(out_channels, out_channels))\n",
    "            return nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.bn1(self.conv1(x)))\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.layer4(x)\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "    \n",
    "    return ResNet50(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Define ResNet-100 Architecture\n",
    "# ============================================================\n",
    "# ResNet-100: 100-layer network with residual connections\n",
    "def create_resnet100(num_classes=10):\n",
    "    \"\"\"ResNet-100 Architecture (Simplified)\"\"\"\n",
    "    class ResNet100(nn.Module):\n",
    "        def __init__(self, num_classes):\n",
    "            super(ResNet100, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "            self.relu = nn.ReLU()\n",
    "            \n",
    "            # More residual blocks for ResNet-100\n",
    "            self.layer1 = self._make_layer(64, 64, 6)\n",
    "            self.layer2 = self._make_layer(64, 128, 8, stride=2)\n",
    "            self.layer3 = self._make_layer(128, 256, 12, stride=2)\n",
    "            self.layer4 = self._make_layer(256, 512, 6, stride=2)\n",
    "            \n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "        def _make_layer(self, in_channels, out_channels, blocks, stride=1):\n",
    "            layers = []\n",
    "            layers.append(create_resnet_block(in_channels, out_channels, stride))\n",
    "            for _ in range(1, blocks):\n",
    "                layers.append(create_resnet_block(out_channels, out_channels))\n",
    "            return nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.bn1(self.conv1(x)))\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.layer4(x)\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "    \n",
    "    return ResNet100(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5febf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: Define EfficientNet Architecture\n",
    "# ============================================================\n",
    "# EfficientNet: Efficient network with balanced scaling\n",
    "def create_efficientnet(num_classes=10):\n",
    "    \"\"\"EfficientNet (Simplified version)\"\"\"\n",
    "    class EfficientNet(nn.Module):\n",
    "        def __init__(self, num_classes):\n",
    "            super(EfficientNet, self).__init__()\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "                \n",
    "                # MBConv blocks (simplified)\n",
    "                nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(),\n",
    "                \n",
    "                nn.Conv2d(16, 24, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(24),\n",
    "                nn.ReLU(),\n",
    "                \n",
    "                nn.Conv2d(24, 40, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(40),\n",
    "                nn.ReLU(),\n",
    "                \n",
    "                nn.Conv2d(40, 80, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(80),\n",
    "                nn.ReLU(),\n",
    "                \n",
    "                nn.Conv2d(80, 112, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(112),\n",
    "                nn.ReLU(),\n",
    "                \n",
    "                nn.AdaptiveAvgPool2d((1, 1))\n",
    "            )\n",
    "            self.classifier = nn.Linear(112, num_classes)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "    \n",
    "    return EfficientNet(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50128078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 10: Define InceptionV3 Architecture\n",
    "# ============================================================\n",
    "# InceptionV3: Network with inception modules (parallel convolutions)\n",
    "def create_inceptionv3(num_classes=10):\n",
    "    \"\"\"InceptionV3 (Simplified)\"\"\"\n",
    "    class InceptionModule(nn.Module):\n",
    "        def __init__(self, in_channels):\n",
    "            super(InceptionModule, self).__init__()\n",
    "            self.branch1 = nn.Conv2d(in_channels, 64, kernel_size=1)\n",
    "            \n",
    "            self.branch2 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, 48, kernel_size=1),\n",
    "                nn.Conv2d(48, 64, kernel_size=3, padding=1)\n",
    "            )\n",
    "            \n",
    "            self.branch3 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, 64, kernel_size=1),\n",
    "                nn.Conv2d(64, 96, kernel_size=3, padding=1),\n",
    "                nn.Conv2d(96, 96, kernel_size=3, padding=1)\n",
    "            )\n",
    "            \n",
    "            self.branch4 = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                nn.Conv2d(in_channels, 32, kernel_size=1)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            branch1 = self.branch1(x)\n",
    "            branch2 = self.branch2(x)\n",
    "            branch3 = self.branch3(x)\n",
    "            branch4 = self.branch4(x)\n",
    "            return torch.cat([branch1, branch2, branch3, branch4], 1)\n",
    "    \n",
    "    class InceptionV3(nn.Module):\n",
    "        def __init__(self, num_classes):\n",
    "            super(InceptionV3, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "            self.conv2 = nn.Conv2d(64, 192, kernel_size=3, padding=1)\n",
    "            self.inception1 = InceptionModule(192)\n",
    "            self.inception2 = InceptionModule(256)\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.fc = nn.Linear(256, num_classes)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = nn.functional.relu(self.conv1(x))\n",
    "            x = nn.functional.max_pool2d(x, 2)\n",
    "            x = nn.functional.relu(self.conv2(x))\n",
    "            x = nn.functional.max_pool2d(x, 2)\n",
    "            x = self.inception1(x)\n",
    "            x = self.inception2(x)\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "    \n",
    "    return InceptionV3(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21833bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 11: Define MobileNet Architecture\n",
    "# ============================================================\n",
    "# MobileNet: Lightweight network for mobile devices\n",
    "def create_mobilenet(num_classes=10):\n",
    "    \"\"\"MobileNet (Simplified)\"\"\"\n",
    "    class MobileNet(nn.Module):\n",
    "        def __init__(self, num_classes):\n",
    "            super(MobileNet, self).__init__()\n",
    "            \n",
    "            def conv_bn(inp, oup, stride):\n",
    "                return nn.Sequential(\n",
    "                    nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "                    nn.BatchNorm2d(oup),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            \n",
    "            def conv_dw(inp, oup, stride):\n",
    "                return nn.Sequential(\n",
    "                    nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "                    nn.BatchNorm2d(inp),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "                    nn.BatchNorm2d(oup),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            \n",
    "            self.model = nn.Sequential(\n",
    "                conv_bn(3, 32, 1),\n",
    "                conv_dw(32, 64, 1),\n",
    "                conv_dw(64, 128, 2),\n",
    "                conv_dw(128, 128, 1),\n",
    "                conv_dw(128, 256, 2),\n",
    "                conv_dw(256, 256, 1),\n",
    "                conv_dw(256, 512, 2),\n",
    "                nn.AdaptiveAvgPool2d(1)\n",
    "            )\n",
    "            self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.model(x)\n",
    "            x = x.view(-1, 512)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "    \n",
    "    return MobileNet(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83690fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 12: Data Loading Function\n",
    "# ============================================================\n",
    "# Function to load and preprocess datasets (MNIST, FashionMNIST, CIFAR-10)\n",
    "def load_dataset(dataset_name='CIFAR10', batch_size=128):\n",
    "    \"\"\"Load dataset with appropriate transforms\"\"\"\n",
    "    \n",
    "    if dataset_name == 'MNIST':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Grayscale(3),  # Convert to 3 channels\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        train_dataset = torchvision.datasets.MNIST(root='./data', train=True, \n",
    "                                                   download=True, transform=transform)\n",
    "        test_dataset = torchvision.datasets.MNIST(root='./data', train=False, \n",
    "                                                  download=True, transform=transform)\n",
    "        num_classes = 10\n",
    "        \n",
    "    elif dataset_name == 'FashionMNIST':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Grayscale(3),\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, \n",
    "                                                          download=True, transform=transform)\n",
    "        test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, \n",
    "                                                         download=True, transform=transform)\n",
    "        num_classes = 10\n",
    "        \n",
    "    else:  # CIFAR10\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                                     download=True, transform=transform_train)\n",
    "        test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, \n",
    "                                                    download=True, transform=transform_test)\n",
    "        num_classes = 10\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return train_loader, test_loader, num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0831a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 13: Define Loss Functions - Focal Loss\n",
    "# ============================================================\n",
    "# Focal Loss: Handles class imbalance by focusing on hard examples\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a5bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 14: Define Loss Functions - ArcFace Loss\n",
    "# ============================================================\n",
    "# ArcFace Loss: Creates better feature separation with angular margin\n",
    "class ArcFaceLoss(nn.Module):\n",
    "    \"\"\"ArcFace Loss for better feature discrimination\"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50):\n",
    "        super(ArcFaceLoss, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, features, labels):\n",
    "        # Normalize features and weights\n",
    "        features = nn.functional.normalize(features, dim=1)\n",
    "        weight = nn.functional.normalize(self.weight, dim=1)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        cosine = nn.functional.linear(features, weight)\n",
    "        \n",
    "        # Add angular margin\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * torch.cos(torch.tensor(self.m)) - sine * torch.sin(torch.tensor(self.m))\n",
    "        \n",
    "        # One-hot encode labels\n",
    "        one_hot = torch.zeros(cosine.size(), device=features.device)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1).long(), 1)\n",
    "        \n",
    "        # Calculate output\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        \n",
    "        return self.ce(output, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5843a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 15: Loss Function Helper\n",
    "# ============================================================\n",
    "# Helper function to get loss function by name\n",
    "def get_loss_function(loss_name, num_classes=10, feature_dim=512):\n",
    "    \"\"\"Get loss function by name\"\"\"\n",
    "    if loss_name == 'BCE' or loss_name == 'CrossEntropy':\n",
    "        return nn.CrossEntropyLoss()\n",
    "    elif loss_name == 'Focal Loss':\n",
    "        return FocalLoss()\n",
    "    elif loss_name == 'ArcFace':\n",
    "        return ArcFaceLoss(feature_dim, num_classes)\n",
    "    else:\n",
    "        return nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbec868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 16: Training Function\n",
    "# ============================================================\n",
    "# Function to train model for one epoch\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, use_arcface=False):\n",
    "    \"\"\"Train model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc='Training', leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_arcface:\n",
    "            # For ArcFace, we need features before final classification\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedcd17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 17: Evaluation Function\n",
    "# ============================================================\n",
    "# Function to evaluate model on test set\n",
    "def evaluate_model(model, test_loader, criterion, device, use_arcface=False):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Testing', leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if use_arcface:\n",
    "                loss = criterion(outputs, labels)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    \n",
    "    return test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a8b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 18: Complete Training Pipeline\n",
    "# ============================================================\n",
    "# Main function to train and evaluate a model\n",
    "def train_and_evaluate(model_name, model, train_loader, test_loader, \n",
    "                      epochs=10, lr=0.001, optimizer_name='Adam', \n",
    "                      loss_name='CrossEntropy', device=device):\n",
    "    \"\"\"Complete training and evaluation pipeline\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"Optimizer: {optimizer_name}, Loss: {loss_name}, Epochs: {epochs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Setup optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Setup loss function\n",
    "    use_arcface = (loss_name == 'ArcFace')\n",
    "    criterion = get_loss_function(loss_name)\n",
    "    criterion = criterion.to(device)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch [{epoch+1}/{epochs}]\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, \n",
    "                                                optimizer, device, use_arcface)\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, criterion, \n",
    "                                            device, use_arcface)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'optimizer': optimizer_name,\n",
    "        'loss_function': loss_name,\n",
    "        'epochs': epochs,\n",
    "        'final_train_acc': train_accs[-1],\n",
    "        'final_test_acc': test_accs[-1],\n",
    "        'training_time': training_time,\n",
    "        'train_losses': train_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'test_losses': test_losses,\n",
    "        'test_accs': test_accs\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "    print(f\"Final Test Accuracy: {test_accs[-1]:.2f}%\")\n",
    "    \n",
    "    return model, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc1b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 19: Plotting Function for Model Comparison\n",
    "# ============================================================\n",
    "# Function to plot comparison of all models\n",
    "def plot_model_comparison(results):\n",
    "    \"\"\"Plot comparison of all models\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    model_names = [r['model_name'] for r in results]\n",
    "    train_accs = [r['final_train_acc'] for r in results]\n",
    "    test_accs = [r['final_test_acc'] for r in results]\n",
    "    times = [r['training_time'] for r in results]\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    axes[0].bar(x - width/2, train_accs, width, label='Train Accuracy')\n",
    "    axes[0].bar(x + width/2, test_accs, width, label='Test Accuracy')\n",
    "    axes[0].set_xlabel('Models')\n",
    "    axes[0].set_ylabel('Accuracy (%)')\n",
    "    axes[0].set_title('Model Accuracy Comparison')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training time comparison\n",
    "    axes[1].bar(model_names, times, color='coral')\n",
    "    axes[1].set_xlabel('Models')\n",
    "    axes[1].set_ylabel('Training Time (seconds)')\n",
    "    axes[1].set_title('Training Time Comparison')\n",
    "    axes[1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('part1_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nComparison plot saved as 'part1_comparison.png'\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca6baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 20: PART 1 - Compare All Models Function\n",
    "# ============================================================\n",
    "# Part 1: Compare all CNN architectures on chosen dataset\n",
    "def part1_compare_all_models(dataset_name='CIFAR10', epochs=10):\n",
    "    \"\"\"Part 1: Compare all CNN architectures\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PART 1: COMPARATIVE ANALYSIS OF CNN ARCHITECTURES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load dataset\n",
    "    train_loader, test_loader, num_classes = load_dataset(dataset_name)\n",
    "    \n",
    "    # Define all models\n",
    "    models_dict = {\n",
    "        'LeNet-5': create_lenet5(num_classes),\n",
    "        'AlexNet': create_alexnet(num_classes),\n",
    "        'VGGNet': create_vggnet(num_classes),\n",
    "        'ResNet-50': create_resnet50(num_classes),\n",
    "        'ResNet-100': create_resnet100(num_classes),\n",
    "        'EfficientNet': create_efficientnet(num_classes),\n",
    "        'InceptionV3': create_inceptionv3(num_classes),\n",
    "        'MobileNet': create_mobilenet(num_classes)\n",
    "    }\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Train each model\n",
    "    for model_name, model in models_dict.items():\n",
    "        _, results = train_and_evaluate(\n",
    "            model_name=model_name,\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            epochs=epochs,\n",
    "            lr=0.001,\n",
    "            optimizer_name='Adam',\n",
    "            loss_name='CrossEntropy',\n",
    "            device=device\n",
    "        )\n",
    "        all_results.append(results)\n",
    "    \n",
    "    # Display comparison table\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARISON RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Model':<15} {'Train Acc':<12} {'Test Acc':<12} {'Time (s)':<12}\")\n",
    "    print(\"-\"*70)\n",
    "    for result in all_results:\n",
    "        print(f\"{result['model_name']:<15} {result['final_train_acc']:>10.2f}% \"\n",
    "              f\"{result['final_test_acc']:>10.2f}% {result['training_time']:>10.2f}\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    plot_model_comparison(all_results)\n",
    "    \n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 21: Plotting Function for Part 2\n",
    "# ============================================================\n",
    "# Function to plot Part 2 results\n",
    "def plot_part2_comparison(results):\n",
    "    \"\"\"Plot Part 2 results\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Group by dataset\n",
    "    mnist_results = [r for r in results if r['dataset'] == 'MNIST']\n",
    "    cifar_results = [r for r in results if r['dataset'] == 'CIFAR10']\n",
    "    \n",
    "    # MNIST results\n",
    "    labels_mnist = [f\"{r['config']['model']}\\n{r['config']['loss']}\" for r in mnist_results]\n",
    "    train_mnist = [r['final_train_acc'] for r in mnist_results]\n",
    "    test_mnist = [r['final_test_acc'] for r in mnist_results]\n",
    "    \n",
    "    x = np.arange(len(labels_mnist))\n",
    "    width = 0.35\n",
    "    axes[0].bar(x - width/2, train_mnist, width, label='Train')\n",
    "    axes[0].bar(x + width/2, test_mnist, width, label='Test')\n",
    "    axes[0].set_xlabel('Model + Loss Function')\n",
    "    axes[0].set_ylabel('Accuracy (%)')\n",
    "    axes[0].set_title('MNIST Dataset Results')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(labels_mnist, fontsize=8)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # CIFAR-10 results\n",
    "    labels_cifar = [f\"{r['config']['model']}\\n{r['config']['loss']}\" for r in cifar_results]\n",
    "    train_cifar = [r['final_train_acc'] for r in cifar_results]\n",
    "    test_cifar = [r['final_test_acc'] for r in cifar_results]\n",
    "    \n",
    "    x = np.arange(len(labels_cifar))\n",
    "    axes[1].bar(x - width/2, train_cifar, width, label='Train')\n",
    "    axes[1].bar(x + width/2, test_cifar, width, label='Test')\n",
    "    axes[1].set_xlabel('Model + Loss Function')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('CIFAR-10 Dataset Results')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(labels_cifar, fontsize=8)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('part2_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nPart 2 comparison plot saved as 'part2_comparison.png'\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74322df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 22: PART 2 - Loss Function Comparison\n",
    "# ============================================================\n",
    "# Part 2: Compare different loss functions and optimizers\n",
    "def part2_loss_function_comparison():\n",
    "    \"\"\"Part 2: Compare different loss functions and optimizers\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PART 2: LOSS FUNCTION AND OPTIMIZER COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Configurations as per table\n",
    "    configurations = [\n",
    "        {'model': 'VGGNet', 'optimizer': 'Adam', 'epochs': 10, 'loss': 'BCE'},\n",
    "        {'model': 'AlexNet', 'optimizer': 'SGD', 'epochs': 20, 'loss': 'Focal Loss'},\n",
    "        {'model': 'ResNet', 'optimizer': 'Adam', 'epochs': 15, 'loss': 'ArcFace'}\n",
    "    ]\n",
    "    \n",
    "    datasets = ['MNIST', 'CIFAR10']\n",
    "    \n",
    "    all_part2_results = []\n",
    "    \n",
    "    for dataset_name in datasets:\n",
    "        print(f\"\\n{'*'*70}\")\n",
    "        print(f\"Testing on {dataset_name} Dataset\")\n",
    "        print(f\"{'*'*70}\")\n",
    "        \n",
    "        train_loader, test_loader, num_classes = load_dataset(dataset_name)\n",
    "        \n",
    "        for config in configurations:\n",
    "            # Create model\n",
    "            if config['model'] == 'VGGNet':\n",
    "                model = create_vggnet(num_classes)\n",
    "            elif config['model'] == 'AlexNet':\n",
    "                model = create_alexnet(num_classes)\n",
    "            else:  # ResNet\n",
    "                model = create_resnet50(num_classes)\n",
    "            \n",
    "            # Train and evaluate\n",
    "            _, results = train_and_evaluate(\n",
    "                model_name=f\"{config['model']} ({dataset_name})\",\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                test_loader=test_loader,\n",
    "                epochs=config['epochs'],\n",
    "                lr=0.001 if config['optimizer'] == 'Adam' else 0.01,\n",
    "                optimizer_name=config['optimizer'],\n",
    "                loss_name=config['loss'],\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            results['dataset'] = dataset_name\n",
    "            results['config'] = config\n",
    "            all_part2_results.append(results)\n",
    "    \n",
    "    # Display results table\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PART 2 RESULTS TABLE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Model':<10} {'Optimizer':<10} {'Epochs':<8} {'Loss Fn':<15} \"\n",
    "          f\"{'Dataset':<10} {'Train Acc':<12} {'Test Acc':<12}\")\n",
    "    print(\"-\"*100)\n",
    "    for result in all_part2_results:\n",
    "        config = result['config']\n",
    "        print(f\"{config['model']:<10} {config['optimizer']:<10} {config['epochs']:<8} \"\n",
    "              f\"{config['loss']:<15} {result['dataset']:<10} \"\n",
    "              f\"{result['final_train_acc']:>10.2f}% {result['final_test_acc']:>10.2f}%\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    plot_part2_comparison(all_part2_results)\n",
    "    \n",
    "    return all_part2_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f222688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 23: Feature Extraction for t-SNE\n",
    "# ============================================================\n",
    "# Function to extract features from model for visualization\n",
    "def extract_features(model, data_loader, device, max_samples=1000):\n",
    "    \"\"\"Extract features from model for visualization\"\"\"\n",
    "    model.eval()\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Get features from second-to-last layer\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            features_list.append(outputs.cpu().numpy())\n",
    "            labels_list.append(labels.numpy())\n",
    "            \n",
    "            if len(features_list) * inputs.size(0) >= max_samples:\n",
    "                break\n",
    "    \n",
    "    features = np.concatenate(features_list, axis=0)[:max_samples]\n",
    "    labels = np.concatenate(labels_list, axis=0)[:max_samples]\n",
    "    \n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea93f478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 24: t-SNE Plotting Function\n",
    "# ============================================================\n",
    "# Function to plot t-SNE visualization\n",
    "def plot_tsne_comparison(models_features):\n",
    "    \"\"\"Plot t-SNE visualization for different loss functions\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(models_features), figsize=(15, 6))\n",
    "    \n",
    "    if len(models_features) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "    \n",
    "    for idx, (loss_name, (features, labels)) in enumerate(models_features.items()):\n",
    "        print(f\"Computing t-SNE for {loss_name}...\")\n",
    "        \n",
    "        # Apply t-SNE\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "        features_2d = tsne.fit_transform(features)\n",
    "        \n",
    "        # Plot\n",
    "        ax = axes[idx]\n",
    "        for class_idx in range(10):\n",
    "            mask = labels == class_idx\n",
    "            ax.scatter(features_2d[mask, 0], features_2d[mask, 1], \n",
    "                      c=[colors[class_idx]], label=f'Class {class_idx}',\n",
    "                      alpha=0.6, s=20)\n",
    "        \n",
    "        ax.set_title(f't-SNE Visualization - {loss_name} Loss', fontsize=12)\n",
    "        ax.set_xlabel('t-SNE Dimension 1')\n",
    "        ax.set_ylabel('t-SNE Dimension 2')\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('part3_tsne_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nt-SNE visualization saved as 'part3_tsne_visualization.png'\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7227e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 25: PART 3 - t-SNE Visualization\n",
    "# ============================================================\n",
    "# Part 3: Visualize feature clustering using t-SNE\n",
    "def part3_tsne_visualization():\n",
    "    \"\"\"Part 3: Visualize feature clustering using t-SNE\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PART 3: t-SNE VISUALIZATION OF LOSS FUNCTIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load CIFAR-10 dataset\n",
    "    train_loader, test_loader, num_classes = load_dataset('CIFAR10')\n",
    "    \n",
    "    # Train models with different loss functions\n",
    "    loss_functions = ['BCE', 'ArcFace']\n",
    "    models_features = {}\n",
    "    \n",
    "    for loss_fn in loss_functions:\n",
    "        print(f\"\\nTraining model with {loss_fn} loss...\")\n",
    "        \n",
    "        model = create_vggnet(num_classes)\n",
    "        model, _ = train_and_evaluate(\n",
    "            model_name=f'VGGNet-{loss_fn}',\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            epochs=5,  # Reduced epochs for faster visualization\n",
    "            lr=0.001,\n",
    "            optimizer_name='Adam',\n",
    "            loss_name=loss_fn,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Extract features\n",
    "        print(f\"Extracting features for {loss_fn}...\")\n",
    "        features, labels = extract_features(model, test_loader, device, max_samples=1000)\n",
    "        models_features[loss_fn] = (features, labels)\n",
    "    \n",
    "    # Create t-SNE visualization\n",
    "    plot_tsne_comparison(models_features)\n",
    "    \n",
    "    return models_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b775c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 26: Main Function to Run All Parts\n",
    "# ============================================================\n",
    "# Main function - Run all parts of the practical\n",
    "def main():\n",
    "    \"\"\"Main function to run all parts\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"DEEP LEARNING PRACTICAL - 3\")\n",
    "    print(\"Comparative Analysis of CNN Architectures\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Choose dataset for Part 1\n",
    "    dataset_choice = 'CIFAR10'  # Options: 'MNIST', 'FashionMNIST', 'CIFAR10'\n",
    "    \n",
    "    # PART 1: Compare all models\n",
    "    print(\"\\nStarting Part 1...\")\n",
    "    part1_results = part1_compare_all_models(dataset_name=dataset_choice, epochs=10)\n",
    "    \n",
    "    # PART 2: Loss function and optimizer comparison\n",
    "    print(\"\\nStarting Part 2...\")\n",
    "    part2_results = part2_loss_function_comparison()\n",
    "    \n",
    "    # PART 3: t-SNE visualization\n",
    "    print(\"\\nStarting Part 3...\")\n",
    "    part3_results = part3_tsne_visualization()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ALL TASKS COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"1. part1_comparison.png - Model comparison plots\")\n",
    "    print(\"2. part2_comparison.png - Loss function comparison\")\n",
    "    print(\"3. part3_tsne_visualization.png - t-SNE feature visualization\")\n",
    "    print(\"\\nCheck the output for detailed results and accuracy metrics.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf77479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 27: RUN THE PRACTICAL\n",
    "# ============================================================\n",
    "# Execute the main function to run all parts\n",
    "# WARNING: This will take 1.5-2 hours to complete on GPU\n",
    "# For quick testing, modify epochs in part1_compare_all_models() and part3_tsne_visualization()\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dfd94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 28 (OPTIONAL): Quick Test - Part 1 Only (3 Models)\n",
    "# ============================================================\n",
    "# Uncomment and run this cell for quick testing with just 3 models\n",
    "# This will complete in about 15-20 minutes\n",
    "\n",
    "\"\"\"\n",
    "print(\"QUICK TEST - Running Part 1 with 3 models only\")\n",
    "train_loader, test_loader, num_classes = load_dataset('CIFAR10')\n",
    "\n",
    "models_dict = {\n",
    "    'LeNet-5': create_lenet5(num_classes),\n",
    "    'VGGNet': create_vggnet(num_classes),\n",
    "    'ResNet-50': create_resnet50(num_classes)\n",
    "}\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for model_name, model in models_dict.items():\n",
    "    _, results = train_and_evaluate(\n",
    "        model_name=model_name,\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        epochs=3,  # Reduced epochs\n",
    "        lr=0.001,\n",
    "        optimizer_name='Adam',\n",
    "        loss_name='CrossEntropy',\n",
    "        device=device\n",
    "    )\n",
    "    all_results.append(results)\n",
    "\n",
    "plot_model_comparison(all_results)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 29 (OPTIONAL): Quick Test - Part 2 Only (MNIST)\n",
    "# ============================================================\n",
    "# Uncomment and run this cell for quick Part 2 testing on MNIST only\n",
    "\n",
    "\"\"\"\n",
    "print(\"QUICK TEST - Running Part 2 on MNIST only\")\n",
    "\n",
    "configurations = [\n",
    "    {'model': 'VGGNet', 'optimizer': 'Adam', 'epochs': 5, 'loss': 'BCE'},\n",
    "    {'model': 'AlexNet', 'optimizer': 'SGD', 'epochs': 5, 'loss': 'Focal Loss'},\n",
    "]\n",
    "\n",
    "train_loader, test_loader, num_classes = load_dataset('MNIST')\n",
    "all_results = []\n",
    "\n",
    "for config in configurations:\n",
    "    if config['model'] == 'VGGNet':\n",
    "        model = create_vggnet(num_classes)\n",
    "    else:\n",
    "        model = create_alexnet(num_classes)\n",
    "    \n",
    "    _, results = train_and_evaluate(\n",
    "        model_name=f\"{config['model']} (MNIST)\",\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        epochs=config['epochs'],\n",
    "        lr=0.001 if config['optimizer'] == 'Adam' else 0.01,\n",
    "        optimizer_name=config['optimizer'],\n",
    "        loss_name=config['loss'],\n",
    "        device=device\n",
    "    )\n",
    "    results['dataset'] = 'MNIST'\n",
    "    results['config'] = config\n",
    "    all_results.append(results)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51732d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 30 (OPTIONAL): Quick Test - Part 3 Only\n",
    "# ============================================================\n",
    "# Uncomment and run this cell for quick Part 3 testing\n",
    "\n",
    "\"\"\"\n",
    "print(\"QUICK TEST - Running Part 3 with 2 epochs\")\n",
    "\n",
    "train_loader, test_loader, num_classes = load_dataset('CIFAR10')\n",
    "loss_functions = ['BCE', 'ArcFace']\n",
    "models_features = {}\n",
    "\n",
    "for loss_fn in loss_functions:\n",
    "    model = create_vggnet(num_classes)\n",
    "    model, _ = train_and_evaluate(\n",
    "        model_name=f'VGGNet-{loss_fn}',\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        epochs=2,  # Reduced epochs\n",
    "        lr=0.001,\n",
    "        optimizer_name='Adam',\n",
    "        loss_name=loss_fn,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    features, labels = extract_features(model, test_loader, device, max_samples=500)\n",
    "    models_features[loss_fn] = (features, labels)\n",
    "\n",
    "plot_tsne_comparison(models_features)\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
